<?xml version="1.0" encoding="UTF-8"?>
<!--
  ~ Licensed to the Apache Software Foundation (ASF) under one or more
  ~ contributor license agreements.  See the NOTICE file distributed with
  ~ this work for additional information regarding copyright ownership.
  ~ The ASF licenses this file to You under the Apache License, Version 2.0
  ~ (the "License"); you may not use this file except in compliance with
  ~ the License.  You may obtain a copy of the License at
  ~
  ~    http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing, software
  ~ distributed under the License is distributed on an "AS IS" BASIS,
  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  ~ See the License for the specific language governing permissions and
  ~ limitations under the License.
  -->

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>
  <parent>
    <groupId>org.apache</groupId>
    <artifactId>apache</artifactId>
    <version>14</version>
  </parent>
  <groupId>org.apache.spark</groupId>
  <artifactId>spark-sql-server_2.12</artifactId>
  <version>0.1.7-spark3.0-SNAPSHOT</version>
  <packaging>pom</packaging>
  <name>Spark SQL Server Parent POM</name>

  <modules>
    <module>sql/sql-server</module>
    <module>examples/tpcds</module>
    <module>examples/extensions</module>
  </modules>

  <properties>
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
    <java.version>1.8</java.version>
    <maven.compiler.source>${java.version}</maven.compiler.source>
    <maven.compiler.target>${java.version}</maven.compiler.target>
    <maven.version>3.6.3</maven.version>
    <scala.version>2.12.10</scala.version>
    <scala.binary.version>2.12</scala.binary.version>
    <spark.version>3.0.0-preview2</spark.version>
    <livy.version>0.5.0-incubating</livy.version>
    <zookeeper.version>3.4.14</zookeeper.version>
    <hadoop.version>2.7.4</hadoop.version>
    <curator.version>2.7.1</curator.version>
    <hadoop.binary.version>2.7</hadoop.binary.version>
    <jetty.version>9.4.18.v20190429</jetty.version>
    <antlr4.version>4.7.1</antlr4.version>
    <scalatest.version>3.0.8</scalatest.version>
    <snappy-java.version>1.1.7.3</snappy-java.version>
    <postgresql.version>42.4.1</postgresql.version>
    <commons-io.version>2.4</commons-io.version>
    <commons-lang3.version>3.9</commons-lang3.version>
    <slf4j.version>1.7.16</slf4j.version>
    <log4j.version>1.2.17</log4j.version>

    <test.java.home>${java.home}</test.java.home>
    <test.exclude.tags></test.exclude.tags>

    <MetaspaceSize>64m</MetaspaceSize>
    <MaxMetaspaceSize>512m</MaxMetaspaceSize>
    <CodeCacheSize>512m</CodeCacheSize>
  </properties>

  <dependencies>
    <dependency>
      <groupId>org.scalatest</groupId>
      <artifactId>scalatest_${scala.binary.version}</artifactId>
      <version>${scalatest.version}</version>
      <scope>test</scope>
    </dependency>
  </dependencies>
  <dependencyManagement>
    <dependencies>
      <!-- Shaded deps marked as provided. These are promoted to compile scope
           in the modules where we want the shaded classes to appear in the
           associated jar. -->
      <dependency>
        <groupId>org.eclipse.jetty</groupId>
        <artifactId>jetty-http</artifactId>
        <version>${jetty.version}</version>
        <scope>provided</scope>
      </dependency>
      <!-- End of shaded deps -->

      <dependency>
        <groupId>org.scala-lang</groupId>
        <artifactId>scala-library</artifactId>
        <version>${scala.version}</version>
        <scope>provided</scope>
      </dependency>
      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-catalyst_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
        <scope>provided</scope>
      </dependency>
      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-sql_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
        <scope>provided</scope>
      </dependency>
      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-hive_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
        <scope>provided</scope>
      </dependency>
      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-core_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
        <scope>provided</scope>
      </dependency>
      <dependency>
        <groupId>org.apache.zookeeper</groupId>
        <artifactId>zookeeper</artifactId>
        <version>${zookeeper.version}</version>
        <scope>provided</scope>
      </dependency>
      <dependency>
        <groupId>org.apache.curator</groupId>
        <artifactId>curator-recipes</artifactId>
        <version>${curator.version}</version>
        <scope>provided</scope>
      </dependency>
      <dependency>
        <groupId>org.antlr</groupId>
        <artifactId>antlr4-runtime</artifactId>
        <version>${antlr4.version}</version>
        <scope>provided</scope>
      </dependency>
      <dependency>
        <groupId>commons-io</groupId>
        <artifactId>commons-io</artifactId>
        <version>${commons-io.version}</version>
        <scope>provided</scope>
      </dependency>
      <dependency>
        <groupId>org.apache.commons</groupId>
        <artifactId>commons-lang3</artifactId>
        <version>${commons-lang3.version}</version>
        <scope>provided</scope>
      </dependency>
      <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-api</artifactId>
        <version>${slf4j.version}</version>
        <scope>provided</scope>
      </dependency>
      <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-log4j12</artifactId>
        <version>${slf4j.version}</version>
        <scope>provided</scope>
      </dependency>
      <dependency>
        <groupId>log4j</groupId>
        <artifactId>log4j</artifactId>
        <version>${log4j.version}</version>
        <scope>provided</scope>
      </dependency>
      <dependency>
        <groupId>org.xerial.snappy</groupId>
        <artifactId>snappy-java</artifactId>
        <version>${snappy-java.version}</version>
        <scope>provided</scope>
      </dependency>

      <dependency>
        <groupId>org.postgresql</groupId>
        <artifactId>postgresql</artifactId>
        <version>${postgresql.version}</version>
      </dependency>

      <dependency>
        <groupId>org.apache.livy</groupId>
        <artifactId>livy-client-http</artifactId>
        <version>${livy.version}</version>
        <scope>compile</scope>
      </dependency>

      <dependency>
        <groupId>org.scalatest</groupId>
        <artifactId>scalatest_${scala.binary.version}</artifactId>
        <version>${scalatest.version}</version>
        <scope>test</scope>
      </dependency>
      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-core_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
        <type>test-jar</type>
        <scope>test</scope>
      </dependency>
      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-catalyst_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
        <type>test-jar</type>
        <scope>test</scope>
      </dependency>
      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-sql_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
        <type>test-jar</type>
        <scope>test</scope>
      </dependency>
      <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-hive_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
        <type>test-jar</type>
        <scope>test</scope>
      </dependency>
      <dependency>
        <groupId>org.apache.curator</groupId>
        <artifactId>curator-test</artifactId>
        <version>${curator.version}</version>
        <scope>test</scope>
      </dependency>
    </dependencies>
  </dependencyManagement>

  <build>
    <directory>target</directory>
    <outputDirectory>target/scala-${scala.binary.version}/classes</outputDirectory>
    <finalName>${project.artifactId}-${project.version}</finalName>
    <testOutputDirectory>target/scala-${scala.binary.version}/test-classes</testOutputDirectory>

    <pluginManagement>
      <plugins>
        <plugin>
          <groupId>org.antlr</groupId>
          <artifactId>antlr4-maven-plugin</artifactId>
          <version>${antlr4.version}</version>
        </plugin>
      </plugins>
    </pluginManagement>

    <plugins>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-enforcer-plugin</artifactId>
        <version>3.0.0-M2</version>
        <executions>
          <execution>
            <id>enforce-versions</id>
            <goals>
              <goal>enforce</goal>
            </goals>
            <configuration>
              <rules>
                <requireMavenVersion>
                  <version>${maven.version}</version>
                </requireMavenVersion>
                <requireJavaVersion>
                  <version>${java.version}</version>
                </requireJavaVersion>
              </rules>
            </configuration>
          </execution>
        </executions>
      </plugin>
      <plugin>
        <groupId>org.scalastyle</groupId>
        <artifactId>scalastyle-maven-plugin</artifactId>
        <version>1.0.0</version>
        <configuration>
          <verbose>false</verbose>
          <failOnViolation>true</failOnViolation>
          <includeTestSourceDirectory>true</includeTestSourceDirectory>
          <failOnWarning>false</failOnWarning>
          <sourceDirectory>src/main/scala</sourceDirectory>
          <testSourceDirectory>src/test/scala</testSourceDirectory>
          <configLocation>scalastyle-config.xml</configLocation>
          <outputFile>target/scalastyle-output.xml</outputFile>
          <inputEncoding>${project.build.sourceEncoding}</inputEncoding>
          <outputEncoding>${project.reporting.outputEncoding}</outputEncoding>
        </configuration>
        <executions>
          <execution>
          <goals>
            <goal>check</goal>
          </goals>
          </execution>
        </executions>
      </plugin>
      <plugin>
        <groupId>net.alchim31.maven</groupId>
        <artifactId>scala-maven-plugin</artifactId>
        <version>4.3.0</version>
        <executions>
          <execution>
            <id>eclipse-add-source</id>
            <goals>
              <goal>add-source</goal>
            </goals>
          </execution>
          <execution>
            <id>scala-compile-first</id>
            <phase>process-resources</phase>
            <goals>
              <goal>compile</goal>
            </goals>
          </execution>
          <execution>
            <id>scala-test-compile-first</id>
            <phase>process-test-resources</phase>
            <goals>
              <goal>testCompile</goal>
            </goals>
          </execution>
          <execution>
            <id>attach-scaladocs</id>
            <phase>verify</phase>
            <goals>
              <goal>doc-jar</goal>
            </goals>
          </execution>
        </executions>
        <configuration>
          <scalaVersion>${scala.version}</scalaVersion>
          <recompileMode>incremental</recompileMode>
          <useZincServer>true</useZincServer>
          <args>
            <arg>-unchecked</arg>
            <arg>-deprecation</arg>
            <arg>-feature</arg>
          </args>
          <jvmArgs>
            <jvmArg>-Xms1024m</jvmArg>
            <jvmArg>-Xmx1024m</jvmArg>
            <jvmArg>-XX:MetaspaceSize=${MetaspaceSize}</jvmArg>
            <jvmArg>-XX:MaxMetaspaceSize=${MaxMetaspaceSize}</jvmArg>
            <jvmArg>-XX:ReservedCodeCacheSize=${CodeCacheSize}</jvmArg>
          </jvmArgs>
          <javacArgs>
            <javacArg>-source</javacArg>
            <javacArg>${java.version}</javacArg>
            <javacArg>-target</javacArg>
            <javacArg>${java.version}</javacArg>
            <javacArg>-Xlint:all,-serial,-path</javacArg>
          </javacArgs>
        </configuration>
      </plugin>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-compiler-plugin</artifactId>
        <version>3.8.1</version>
        <configuration>
          <source>${java.version}</source>
          <target>${java.version}</target>
          <encoding>UTF-8</encoding>
          <maxmem>1024m</maxmem>
          <fork>true</fork>
          <compilerArgs>
            <arg>-Xlint:all,-serial,-path</arg>
          </compilerArgs>
        </configuration>
      </plugin>
      <!-- Surefire runs all Java tests -->
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-surefire-plugin</artifactId>
        <version>3.0.0-M3</version>
        <!-- Note config is repeated in scalatest config -->
        <configuration>
          <includes>
            <include>**/Test*.java</include>
            <include>**/*Test.java</include>
            <include>**/*TestCase.java</include>
            <include>**/*Suite.java</include>
          </includes>
          <reportsDirectory>${project.build.directory}/surefire-reports</reportsDirectory>
          <argLine>-Xmx3g -Xss4096k -XX:MaxMetaspaceSize=${MaxMetaspaceSize} -XX:ReservedCodeCacheSize=512m</argLine>
          <environmentVariables>
            <!--
              Setting SPARK_DIST_CLASSPATH is a simple way to make sure any child processes
              launched by the tests have access to the correct test-time classpath.
            -->
            <SPARK_DIST_CLASSPATH>${test_classpath}</SPARK_DIST_CLASSPATH>
            <SPARK_PREPEND_CLASSES>1</SPARK_PREPEND_CLASSES>
            <SPARK_SCALA_VERSION>${scala.binary.version}</SPARK_SCALA_VERSION>
            <SPARK_TESTING>1</SPARK_TESTING>
            <JAVA_HOME>${test.java.home}</JAVA_HOME>
          </environmentVariables>
          <systemProperties>
            <log4j.configuration>file:src/test/resources/log4j.properties</log4j.configuration>
            <derby.system.durability>test</derby.system.durability>
            <java.awt.headless>true</java.awt.headless>
            <java.io.tmpdir>${project.build.directory}/tmp</java.io.tmpdir>
            <spark.test.home>${spark.test.home}</spark.test.home>
            <spark.testing>1</spark.testing>
            <spark.master.rest.enabled>false</spark.master.rest.enabled>
            <spark.ui.enabled>false</spark.ui.enabled>
            <spark.ui.showConsoleProgress>false</spark.ui.showConsoleProgress>
            <spark.unsafe.exceptionOnMemoryLeak>true</spark.unsafe.exceptionOnMemoryLeak>
            <spark.memory.debugFill>true</spark.memory.debugFill>
            <!-- Needed by sql/hive tests. -->
            <test.src.tables>src</test.src.tables>
          </systemProperties>
          <failIfNoTests>false</failIfNoTests>
          <excludedGroups>${test.exclude.tags}</excludedGroups>
        </configuration>
        <executions>
          <execution>
            <id>test</id>
            <goals>
              <goal>test</goal>
            </goals>
          </execution>
        </executions>
      </plugin>
      <!-- Scalatest runs all Scala tests -->
      <plugin>
        <groupId>org.scalatest</groupId>
        <artifactId>scalatest-maven-plugin</artifactId>
        <version>2.0.0</version>
        <!-- Note config is repeated in surefire config -->
        <configuration>
          <reportsDirectory>${project.build.directory}/surefire-reports</reportsDirectory>
          <junitxml>.</junitxml>
          <filereports>SparkTestSuite.txt</filereports>
          <argLine>-ea -Xmx3g -XX:MaxMetaspaceSize=${MaxMetaspaceSize} -XX:ReservedCodeCacheSize=${CodeCacheSize}</argLine>
          <stderr/>
          <environmentVariables>
            <!--
              Setting SPARK_DIST_CLASSPATH is a simple way to make sure any child processes
              launched by the tests have access to the correct test-time classpath.
            -->
            <SPARK_DIST_CLASSPATH>${test_classpath}</SPARK_DIST_CLASSPATH>
            <SPARK_PREPEND_CLASSES>1</SPARK_PREPEND_CLASSES>
            <SPARK_SCALA_VERSION>${scala.binary.version}</SPARK_SCALA_VERSION>
            <SPARK_TESTING>1</SPARK_TESTING>
            <JAVA_HOME>${test.java.home}</JAVA_HOME>
          </environmentVariables>
          <systemProperties>
            <log4j.configuration>file:src/test/resources/log4j.properties</log4j.configuration>
            <derby.system.durability>test</derby.system.durability>
            <java.awt.headless>true</java.awt.headless>
            <java.io.tmpdir>${project.build.directory}/tmp</java.io.tmpdir>
            <spark.test.home>${spark.test.home}</spark.test.home>
            <spark.testing>1</spark.testing>
            <spark.ui.enabled>false</spark.ui.enabled>
            <spark.ui.showConsoleProgress>false</spark.ui.showConsoleProgress>
            <spark.unsafe.exceptionOnMemoryLeak>true</spark.unsafe.exceptionOnMemoryLeak>
            <!-- Needed by sql/hive tests. -->
            <test.src.tables>__not_used__</test.src.tables>
          </systemProperties>
          <tagsToExclude>${test.exclude.tags}</tagsToExclude>
        </configuration>
        <executions>
          <execution>
            <id>test</id>
            <goals>
              <goal>test</goal>
            </goals>
          </execution>
        </executions>
      </plugin>
      <plugin>
        <groupId>org.codehaus.mojo</groupId>
        <artifactId>cobertura-maven-plugin</artifactId>
        <version>2.7</version>
        <configuration>
          <instrumentation>
            <excludes>
              <exclude>**/*$*</exclude>
              <exclude>**/*$</exclude>
              <exclude>org/apache/spark/sql/server/ui/**</exclude>
              <exclude>org/apache/spark/sql/server/parser/**</exclude>
              <exclude>org/apache/spark/sql/server/execution/**</exclude>
              <exclude>org/apache/spark/sql/server/benchmark/**</exclude>
            </excludes>
          </instrumentation>
          <format>xml</format>
          <maxmem>256m</maxmem>
          <!-- aggregated reports for multi-module projects -->
          <aggregate>true</aggregate>
        </configuration>
      </plugin>
      <plugin>
        <groupId>org.eluder.coveralls</groupId>
        <artifactId>coveralls-maven-plugin</artifactId>
        <version>4.3.0</version>
        <!-- For local uses
        <configuration>
          <repoToken></repoToken>
        </configuration>
        -->
      </plugin>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-jar-plugin</artifactId>
        <version>3.2.0</version>
        <executions>
          <execution>
            <!--
              To avoid the error below, `<id>default-jar</id>` was added by checking a link below:
               - https://stackoverflow.com/questions/40964500/maven-jar-plugin-3-0-2-error-you-have-to-use-a-classifier-to-attach-supplementa
              [ERROR] Failed to execute goal org.apache.maven.plugins:maven-jar-plugin:3.2.0:jar (default) on project sql-server_2.12: You have to use a classifier to attach supplemental artifacts to the project instead of replacing them. -> [Help 1]
             -->
            <id>default-jar</id>
            <phase>package</phase>
            <goals>
              <goal>jar</goal>
            </goals>
          </execution>
          <!-- Build test-jar's for all projects, since some projects depend on tests from others -->
          <execution>
            <id>prepare-test-jar</id>
            <phase>prepare-package</phase>
            <goals>
              <goal>test-jar</goal>
            </goals>
            <configuration>
              <excludes>
                <exclude>log4j.properties</exclude>
              </excludes>
            </configuration>
          </execution>
        </executions>
        <configuration>
          <finalName>${project.artifactId}_${spark.version}_${project.version}</finalName>
          <outputDirectory>${project.parent.build.directory}</outputDirectory>
        </configuration>
      </plugin>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-shade-plugin</artifactId>
        <version>3.2.1</version>
          <executions>
            <execution>
            <id>jar-with-dependencies</id>
            <phase>package</phase>
            <goals>
              <goal>shade</goal>
            </goals>
            <configuration>
              <finalName>${project.artifactId}_${spark.version}_${project.version}-with-dependencies</finalName>
              <outputDirectory>${project.parent.build.directory}</outputDirectory>
              <minimizeJar>false</minimizeJar>
              <createDependencyReducedPom>false</createDependencyReducedPom>
              <artifactSet>
                <includes></includes>
              </artifactSet>
            </configuration>
          </execution>
        </executions>
      </plugin>
      <!-- This plugin needed to build in IntelliJ -->
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-resources-plugin</artifactId>
        <version>3.1.0</version>
      </plugin>
    </plugins>
  </build>
</project>
